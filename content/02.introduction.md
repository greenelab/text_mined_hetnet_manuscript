## Introduction

Knowledge bases are important resources that hold complex structured and unstructured information. 
These resources have been used in important tasks such as network analysis for drug repurposing discovery [@doi:10.1371/journal.pone.0084912; @doi:10.1101/385617; @doi:10.7554/eLife.26726] or as a source of training labels for text mining systems [@doi:10.3115/1690219.1690287; @doi:10.1101/444398; @doi:10.1186/s12859-019-2873-7]. 
Populating knowledge bases often requires highly trained scientists to read biomedical literature and summarize the results [@doi:10.1093/bib/bbn043].
This time-consuming process is referred to as manual curation.
In 2007, researchers estimated that filling a knowledge base via manual curation would require approximately 8.4 years to complete [@doi:10.1093/bioinformatics/btm229]. 
The rate of publications continues to exponentially increase [@doi:10.1002/asi.23329], so using only manual curation to fully populate a knowledge base has become impractical.  

Relationship extraction has been studied as a solution towards handling the challenge posed by an exponentially growing body of literature [@doi:10.1093/bib/bbn043].
This process consists of creating an expert system to automatically scan, detect and extract relationships from textual sources.
Typically, these systems utilize machine learning techniques that require extensive corpora of well-labeled training data.
These corpora are difficult to obtain, because they are constructed via extensive manual curation pipelines.  

Distant supervision is a technique also designed to sidestep the dependence on manual curation and quickly generate large training datasets.
This technique assumes that positive examples established in selected databases can be applied to any sentence that contains them [@doi:10.3115/1690219.1690287].
The central problem with this technique is that generated labels are often of low quality which results in an expansive amount of false positives [@raw:Jiang2018RevisitingDS].  

Ratner et al. [@arxiv:1605.07723] recently introduced "data programming" as a solution.
Data programming is a paradigm that combines distant supervision with simple rules and heuristics written as small programs called label functions.
These label functions are consolidated via a noise aware generative model that is designed to produce training labels for large datasets.
Using this paradigm can dramatically reduce the time required to obtain sufficient training data; however, writing a useful label function requires a significant amount of time and error analysis.
This dependency makes constructing a knowledge base with a myriad of heterogenous relationships nearly impossible as tens or possibly hundreds of label functions are required per relationship type.  

In this paper, we seek to accelerate the label function creation process by measuring the extent to which label functions can be re-used across different relationship types.
We hypothesize that sentences describing one relationship type may share linguistic features such as keywords or sentence structure with sentences describing other relationship types.
We conducted a series of experiments to determine the degree to which label function re-use enhanced performance over distant supervision alone.
We focus on relationships that indicate similar types of physical interactions (i.e., gene-binds-gene and compound-binds-gene) as well as different types (i.e., disease-associates-gene and compound-treats-disease).
Re-using label functions could dramatically reduce the time required to populate a knowledge base with a multitude of heterogeneous relationships.

### Related Work

Relationship extraction is the process of detecting semantic relationships from a collection of text.
This process can be broken down into three different categories: (1) the use of natural language processing techniques such as manually crafted rules and heuristics for relationship extraction (Rule Based Extractors), (2) the use of unsupervised methods such as co-occurrence scores or clustering to find patterns within sentences and documents (Unsupervised Extractors), and (3) the use of supervised or semi-supervised machine learning for classifying the presence of a relation within documents or sentences (Supervised Extractors).
In this section, we briefly discuss selected efforts under each category.

#### Rule Based Extractors

Rule based extractors rely heavily on expert knowledge to perform extraction.
Typically, these systems use linguistic rules and heuristics to identify key sentences or phrases.
For example, a hypothetical extractor focused on protein phosphorylation events would identify sentences containing the phrase "gene X phosphorylates gene Y" [@doi:10.1109/TCBB.2014.2372765].
This phrase is a straightforward indication that two genes have a fundamental role in protein phosphorylation.
Other phrase extractors have been used to identify drug-disease treatments [@tag:ctd_medline], pharmcogenomic events [@tag:pharmpresso] and protein-protein interactions [@tag:ppinterfinder; @tag:hpiminer].
These extractors provide a simple and effective way to extract sentences; however, they depend on extensive knowledge about the text to be properly constructed.

A sentence's grammatical structure can also support relationship extraction via dependency trees.
Dependency trees are data structures that depict a sentence's grammatical relation structure in the form of nodes and edges.
Nodes represent words and edges represent the dependency type each word shares between one another.
For example, a possible extractor would classify sentences as a positive if a sentence contained the following dependency tree path: "gene X (subject)-> promotes (verb)<- cell death (direct object) <- in (preposition) <-tumors (object of preposition)" [@tag:pkde4j].
This approach provides extremely precise results, but the quantity of positive results remains modest as sentences appear in distinct forms and structure.
Because of this limitation, recent approaches have incorporated methods on top of rule based extractors such as co-occurrence and machine learning systems [@doi:10.1186/s12859-018-2103-8; @tag:limtox].
We discuss the pros and cons of added methods in a later section.
For this project, we constructed our label functions without the aid of these works; however, approaches discussed in this section provide substantial inspiration for novel label functions in future endeavors.

#### Unsupervised Extractors

Unsupervised extractors detect relationships without the need of annotated text.
Notable approaches exploit the fact that two entities can occur together in text.
This event is referred to as co-occurrence.
Extractors utilize these events by generating statistics on the frequency of entity pairs occurring in text.
For example, a possible extractor would say gene X is associated with disease Y, because gene X and disease Y appear together more often than individually [@tag:diseases].
This approach has been used to establish the following relationship types: disease-gene relationships [@tag:diseases; @tag:polysearch; @tag:dg_text_pubmed; @tag:lgscore; @tag:full_text_co_abstracts; @tag:copub_discovery], protein-protein interactions [@tag:protein_protein_co_network; @doi:10.1093/database/bau012; @tag:full_text_co_abstracts], drug-disease treatments [@tag:abc_drugs], and tissue-gene relations [@doi:10.7717/peerj.1054].
Extractors using the co-occurrence strategy provide exceptional recall results; however, these methods may fail to detect underreported relationships, because they depend on entity-pair frequency for detection.
Junge et al. created a hybrid approach to account for this issue using distant supervision to train a classifier to learn the context of each sentence [@tag:cocoscore].
Once the classifier was trained, they scored every sentence within their corpus, and each sentence's score was incorporated into calculating co-occurrence frequencies to establish relationship existence [@tag:cocoscore].
Co-occurrence approaches are powerful in establishing edges on the global scale; however, they cannot identify individual sentences without the need for supervised methods.  

Clustering is an unsupervised approach that extracts relationships from text by grouping similar sentences together.
Percha et al. used this technique to group sentences based on their grammatical structure [@tag:global_network].
Using Stanford's Core NLP Parser [@doi:10.3115/v1/P14-5010], a dependency tree was generated for every sentence in each Pubmed abstract [@tag:global_network].
Each tree was clustered based on similarity and each cluster was manually annotated to determine which relationship each group represented [@tag:global_network].
For our project we incorporated the results of this work as domain heuristic label functions.
Overall, unsupervised approaches are desirable since they do not require well-annotated training data. 
Such approaches provide excellent recall; however, performance can be limited in terms of precision when compared to supervised machine learning methods [@doi:10.1038/nrg1768; @doi:10.1016/j.ymeth.2015.01.015].

#### Supervised Extractors

Supervised extractors consist of training a machine learning classifier to predict the existence of a relationship within text.
These classifiers require access to well-annotated datasets, which are usually created via some form of manual curation.
Previous work consists of research experts curating their own datasets to train classifiers [@tag:befree; @tag:eu_adr; @tag:aimed; @tag:bioinfer; @tag:hprd50]; however, there have been community-wide efforts to create datasets for shared tasks [@tag:biocreative_v; @raw:biocreative/chemprot; @doi:10.1186/1471-2105-9-S3-S6].
Shared tasks are open challenges that aim to build the best classifier for natural language processing tasks such as named entity tagging or relationship extraction. 
A notable example is the BioCreative community that hosted a number of shared tasks such as predicting compound-protein interactions (BioCreative VI track 5) [@raw:biocreative/chemprot] and compound induced diseases [@doi:10.1186/1471-2105-9-S3-S6].
Often these datasets are well annotated, but are modest in size (2,432 abstracts  for BioCreative VI [@raw:biocreative/chemprot] and 1500 abstracts for BioCreative V [@doi:10.1186/1471-2105-9-S3-S6]).
As machine learning classifiers become increasingly complex, these small dataset sizes cannot suffice.
Plus, these multitude of datasets are uniquely annotated which can generate noticeable differences in terms of classifier performance [@doi:10.1186/1471-2105-9-S3-S6].
Overall, obtaining large well-annotated datasets still remains as an open non-trivial task.

Before the rise of deep learning, a classifier that was most frequently used was support vector machines.
This classifier uses a projection function called a kernel to map data onto a high dimensional space so datapoints can be easily discerned between classes [@doi:10.1109/5254.708428].
This method was used to extract disease-gene associations [@tag:befree; @tag:dtminer; @tag:ensemble_svm], protein-protein interactions[@tag:ppi_graph_kernels; @tag:limtox; @tag:lptk] and protein docking information [@tag:protein_docking].
Generally, support vector machines perform well on small datasets with large feature spaces but are slow to train as the number of datapoints becomes asymptotically large.

Deep learning has been increasingly popular as these methods can outperform common machine learning methods [@doi:10.1016/j.neunet.2014.09.003].
Approaches in this field consist of using various neural network architectures, such as recurrent neural networks [@tag:ppi_bilstm; @tag:cbg_ensemble_dl; @tag:cbg_neural_attention; @tag:recursive_nn; @tag:semi_supervised_vae; @tag:biobert] and convolutional neural networks [@tag:ppi_deep_conv; @tag:mcdepcnn; @tag:cbg_ensemble_dl;@tag:semi_supervised_vae; @tag:cbg_transfer_learning], to extract relationships from text.
In fact approaches in this field were the winning model within the BioCreative VI shared task [@raw:biocreative/chemprot; @raw:chemprot_winner].
Despite the substantial success of these models, they often require large amounts of data to perform well.
Obtaining large datasets is a time-consuming task, which makes training these models a non-trivial challenge.
Distant supervision has been used as a solution to fix the barren amount of large datasets [@doi:10.3115/1690219.1690287].
Approaches have used this paradigm to extract chemical-gene interactions [@tag:semi_supervised_vae], disease-gene associations [@tag:cocoscore] and protein-protein interactions [@tag:deep_dive; @tag:cocoscore; @tag:semi_supervised_vae].
In fact, efforts done in [@tag:deep_dive] served as one of the motivating rationales for our work.  

Overall, deep learning has provided exceptional results in terms of relationships extraction.
Thus, we decided to use a deep neural network as our discriminative model.


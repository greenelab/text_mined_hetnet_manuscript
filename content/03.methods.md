<style> 
span.gene_color { color:#02b3e4 } 
span.disease_color { color:#875442 } 
span.compound_color { color:#e91e63 }
 </style> 

## Methods and Materials

### Hetionet

Hetionet v1 [@doi:10.7554/eLife.26726] is a heterogeneous network that contains pharmacological and biological information.
This network depicts information in the form of nodes and edges of different types.
Nodes in this network represent biological and pharmacological entities, while edges represent relationships between entities.
Hetionet v1 contains 47,031 nodes with 11 different data types and 2,250,197 edges that represent 24 different relationship types (Figure {@fig:hetionet}).
Edges in Hetionet v1 were obtained from open databases, such as the GWAS Catalog [@doi:10.1093/nar/gkw1133], Human Interaction database [@doi:10.1016/j.cell.2014.10.050] and DrugBank [@doi:10.1093/nar/gkx1037].
For this project, we analyzed performance over a subset of the Hetionet v1 edge types: disease associates with a gene (DaG), compound binds to a gene (CbG), compound treating a disease (CtD), and gene interacts with gene (GiG) (bolded in Figure {@fig:hetionet}).

![
A metagraph (schema) of Hetionet v1 where biomedical entities are represented as nodes and the relationships between them are represented as edges.
We examined performance on the highlighted subgraph; however, the long-term vision is to capture edges for the entire graph.
](images/figures/hetionet/metagraph_highlighted_edges.png){#fig:hetionet}

### Dataset

We used PubTator Central [@doi:10.1093/nar/gkz389] as input to our analysis.
PubTator Central provides MEDLINE abstracts that have been annotated with well-established entity recognition tools including Tagger One [@doi:10.1093/bioinformatics/btw343] for disease, chemical and cell line entities, tmVar [@doi:10.1093/bioinformatics/btx541] for genetic variation tagging, GNormPlus [@doi:10.1155/2015/918710] for gene entities and SR4GN [@doi:10.1371/journal.pone.0038460] for species entities.
We downloaded PubTator Central on March 1, 2020, at which point it contained approximately 30,000,000 documents.
After downloading, we filtered out annotated entities that were not contained in Hetionet v1.
We extracted sentences with two or more annotations and termed these sentences as candidate sentences.
We used the Spacy's English natural language processing (NLP) pipeline (en\_core\_web\_sm) [@spacy2] to generate dependency trees and parts of speech tags for every extracted candidate sentence.
Each candidate sentence was stratified by their corresponding abstract ID to produce a training set, tuning set, and a testing set.
We used random assortment to assign dataset labels to each abstract. 
Every abstract had a 70% chance of being labeled training, 20% chance of being labeled tuning, and 10% chance of being labeled testing.
Despite the power of data programming, all text mining systems need to have ground truth labels to be well-calibrated.
We hand-labeled five hundred to a thousand candidate sentences of each edge type to obtain a ground truth set (Table {@tbl:candidate-sentences}).

| Relationship | Train | Tune | Test |
| :--- | :---: | :---: | :---: |
| Disease-associates-Gene (DaG) | 2.49 M | 696K (397+, 603-) | 348K (351+, 649-) |
| Compound-binds-Gene (CbG) | 2.4M | 684K (37+, 463-) | 341k (31+, 469-) |
| Compound-treats-Disease (CtD) | 1.5M | 441K (96+, 404-) | 223K (112+, 388-) |
| Gene-interacts-Gene (GiG) | 11.2M | 2.19M (60+, 440-) | 1.62M (76+, 424-) |

Table: Statistics of Candidate Sentences. 
We sorted each abstract into a training, tuning and testing set.
Numbers in parentheses show the number of positives and negatives that resulted from the hand-labeling process.
{#tbl:candidate-sentences}


### Label Functions for Annotating Sentences

The challenge of having too few ground truth annotations is familiar to many natural language processing applications, even when unannotated text is abundant.
Data programming circumvents this issue by quickly annotating large datasets using multiple noisy signals emitted by label functions [@arxiv:1605.07723].
Label functions are simple pythonic functions that emit: a positive label (1), a negative label (0), or abstain from emitting a label (-1).
These functions can use different approaches or techniques to emit a label; however, these functions can be grouped into simple categories discussed below.
Once constructed, these functions are combined using a generative model to output a single annotation. 
This single annotation is a consensus probability score bounded between 0 (low chance of mentioning a relationship) and 1 (high chance of mentioning a relationship).
We used these annotations to train a discriminative model for the final classification step.

#### Label Function Categories

Label functions can be constructed in various ways; however, they also share similar characteristics.
We grouped functions into databases and text patterns.
The majority of our label functions fall into the text pattern category (Supplemental Table {@tbl:label-functions}).
Further, we described each label function category and provided an example that refers to the following candidate sentence: "[PTK6]{.gene_color} may be a novel therapeutic target for [pancreatic cancer]{.disease_color}".

**Databases**: These label functions incorporate existing databases to generate a signal, as seen in distant supervision [@doi:10.3115/1690219.1690287].
These functions detect if a candidate sentence's co-mention pair is present in a given database.
Our label function emits a positive label if the pair is present and abstains otherwise.
If the pair is not present in any existing database, a separate label function emits a negative label.
We used a separate label function to prevent a label imbalance problem, which can occur when a single function labels every possible sentence despite being correct or not.
If this problem isn't handled correctly, the generative model could become biased and only emit one prediction (solely positive or solely negative) for every sentence.

$$ \Lambda_{DB}(\color{#875442}{D}, \color{#02b3e4}{G}) = 
\begin{cases}
 1 & (\color{#875442}{D}, \color{#02b3e4}{G}) \in DB \\
0 & otherwise \\
\end{cases} $$

$$ \Lambda_{\neg DB}(\color{#875442}{D}, \color{#02b3e4}{G}) = 
\begin{cases}
 -1 & (\color{#875442}{D}, \color{#02b3e4}{G}) \notin DB \\
0 & otherwise \\
\end{cases} $$


**Text Patterns**: These label functions are designed to use keywords or sentence context to generate a signal. 
For example, a label function could focus on the number of words between two mentions and emit a label if two mentions are too close.
Alternatively, a label function could focus on the parts of speech contained within a sentence and ensures a verb is present.
Besides parts of speech, a label function could exploit dependency parse trees to emit a label.
These trees are akin to the tree data structure where words are nodes and edges are how each word modifies each other.
Label functions that use these parse trees will test if the generated tree matches a pattern and emits a positive label if true.
For our analysis, we used previously identified patterns designed for biomedical text to generate our label functions [@doi:10.1093/bioinformatics/bty114].

$$ \Lambda_{TP}(\color{#875442}{D}, \color{#02b3e4}{G}) = 
\begin{cases}
 1 & "target" \> \in Candidate \> Sentence \\
 -1 & otherwise \\
\end{cases} $$

$$ \Lambda_{TP}(\color{#875442}{D}, \color{#02b3e4}{G}) = 
\begin{cases}
 0 & 	"VB" \> \notin pos\_tags(Candidate \> Sentence) \\
 -1 & otherwise \\
\end{cases} $$

$$
\Lambda_{TP}(\color{#875442}{D}, \color{#02b3e4}{G}) = \begin{cases}
    1 & dep(Candidate \> Sentence) \in Cluster \> Theme\\
    -1 & otherwise \\
    \end{cases}
$$

Each text pattern label function was constructed via manual examination of sentences within the training set.
For example, using the candidate sentence above, one would identify the phrase "novel therapeutic target" and incorporate this phrase into a global list that a label function would use to check if present in a sentence.
After initial construction, we tested and augmented the label function using sentences in the tune set.
We repeated this process for every label function in our repertoire. 

| Relationship | Databases (DB) | Text Patterns (TP)
| --- | :---: | :---: |
| DaG | 7 | 30 | 
| CtD | 3 | 22 |
| CbG | 9 | 20 |
| GiG | 9 | 28 |

Table: The distribution of each label function per relationship. {#tbl:label-functions} 

### Training Models

#### Generative Model

The generative model is a core part of this automatic annotation framework.
It integrates multiple signals emitted by label functions to assign each candidate sentence the most appropriate training class.
This model takes as input a label function output in the form of a matrix where rows represent candidate sentences, and columns represent each label function ($\Lambda^{nxm}$).
Once constructed, this model treats the true training class ($Y$) as a latent variable and assumes that each label function is independent of one another.
Under these two assumptions, the model finds the optimal parameters by minimizing a loglikelihood function marginalized over the latent training class.

$$
\hat{\theta} = argmin_{\theta}\sum_{Y}-log(P_{\theta}(\Lambda, Y)) 
$$

Following optimization, the model emits a probability estimate that each sentence belongs to the positive training class.
<div style="color:red">
Insert the parameter search for the generative model and explain how we found the optimal parameters.
</div>
At this step, each probability estimate can be discretized via a chosen threshold into a positive or negative class.
We used a threshold of 0.5 for discretizing our training classes within our analysis.
For more information on how the likelihood function is constructed and minimized, refer to [@doi:10.1007/s00778-019-00552-1].

#### Discriminative Model

The discriminative model is the final step in this framework.
This model uses training labels generated from the generative model combined with sentence features to classify the presence of a biomedical relationship.
Typically, the discriminative model is a neural network.
We used BioBERT [@arxiv:1901.08746], a BERT [@BERT] model trained on all papers and abstracts within Pubmed Central [@doi:10.1073/pnas.98.2.381], as our discriminative model.
<div style="color:red">
Why did we choose biobert?
</div>
BioBERT provides its own set of word embeddings, dense vectors representing words that models such as neural networks can use to construct sentence features.
We downloaded a pre-trained version of this model using huggingface's transformer python package [@Wolf_Transformers_State-of-the-Art_Natural_2020] and fine-tuned it using our generated training labels.
Our fine-tuning approach involved freezing all downstream layers except for the classification head of this model.
<div style="color:red">
I don't think we performed any in-depth hyperparameter search so I'll explain due to computational constraints.
</div>
Next, we trained this model for 10 epochs using the Adam optimizer [@arxiv:1412.6980] with huggingface's default parameter settings and a learning rate of 0.001.

### Experimental Design

Reusing label functions across edge types would substantially reduce the number of label functions required to extract multiple relationships from biomedical literature.
We first established a baseline by training a generative model using only distant supervision label functions designed for the target edge type (see Supplemental Methods).
Then we compared the baseline model with models that incorporated a set number of text pattern label functions.
Using a sampling with replacement approach, we sampled these text pattern label functions from three different groups: within edge types, across edge types, and from a pool of all label functions.
We compared within-edge-type performance to across-edge-type and all-edge-type performance.
We sampled a fixed number of label functions for each edge type consisting of five evenly spaced numbers between one and the total number of possible label functions.
We repeated this sampling process 50 times for each point.
Furthermore, we also trained the discriminative model using annotations from the generative model trained on edge-specific label functions at each point.
We report the performance of both models in terms of the area under the receiver operating characteristic curve (AUROC) and the area under the precision-recall curve (AUPR).
<div style="color:red">
Insert along with reporting the AUROC/AUPR that we report values in the terms of bootstrapping.
</div>
Ensuing model evaluations, we quantified the number of edges we could incorporate into Hetionet v1.
We used our best performing discriminative model to score every candidate sentence within our dataset and grouped candidates based on their mention pair. 
We took the max score within each candidate group, and this score represents the probability of the existence of an edge. 
We established edges using a cutoff score that produced an equal error rate between the false positives and false negatives.
Lastly, we report the number of preexisting edges we could recall and the number of novel edges we can incorporate.

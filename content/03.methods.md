<style> 
span.gene_color { color:#02b3e4 } 
span.disease_color { color:#875442 } 
span.compound_color { color:#e91e63 }
 </style> 

## Materials and Methods

### Hetionet

![
A metagraph (schema) of Hetionet where biomedical entities are represented as nodes and the relationships between them are represented as edges.
We examined performance on the highlighted subgraph; however, the long-term vision is to capture edges for the entire graph.
](images/figures/hetionet/metagraph_highlighted_edges.png){#fig:hetionet}

Hetionet [@doi:10.7554/eLife.26726] is a large heterogenous network that contains pharmacological and biological information.
This network depicts information in the form of nodes and edges of different types: nodes that represent biological and pharmacological entities and edges which represent relationships between entities. 
Hetionet v1.0 contains 47,031 nodes with 11 different data types and 2,250,197 edges that represent 24 different relationship types (Figure {@fig:hetionet}).
Edges in Hetionet were obtained from open databases, such as the GWAS Catalog [@doi:10.1093/nar/gkw1133] and DrugBank [@doi:10.1093/nar/gkx1037].
For this project, we analyzed performance over a subset of the Hetionet relationship types: disease associates with a gene (DaG), compound binds to a gene (CbG), gene interacts with gene (GiG) and compound treating a disease (CtD).

### Dataset

We used PubTator [@doi:10.1093/nar/gkt441] as input to our analysis.
PubTator provides MEDLINE abstracts that have been annotated with well-established entity recognition tools including DNorm [@doi:10.1093/bioinformatics/btt474] for disease mentions, GeneTUKit [@doi:10.1093/bioinformatics/btr042] for gene mentions, Gnorm [@doi:10.1186/1471-2105-12-S8-S5] for gene normalizations and a dictionary based search system for compound mentions [@doi:10.1093/database/bas037].
We downloaded PubTator on June 30, 2017, at which point it contained 10,775,748 abstracts. 
Then we filtered out mention tags that were not contained in hetionet.
We used the Stanford CoreNLP parser [@doi:10.3115/v1/P14-5010] to tag parts of speech and generate dependency trees.
We extracted sentences with two or more mentions, termed candidate sentences.
Each candidate sentence was stratified by co-mention pair to produce a training set, tuning set and a testing set (shown in Table {@tbl:candidate-sentences}).
Each unique co-mention pair is sorted into four categories: (1) in hetionet and has sentences, (2) in hetionet and doesn't have sentences, (3) not in hetionet and does have sentences and (4) not in hetionet and doesn't have sentences.
Within these four categories each pair is randomly assigned their own individual partition rank (continuous number between 0 and 1).
Any rank lower than 0.7 is sorted into the training set, while any rank greater than 0.7 and lower than 0.9 is assigned to the tuning set.
The rest of the pairs with a rank greater than or equal to 0.9 is assigned to the test set.
Sentences that contain more than one co-mention pair are treated as multiple individual candidates.
We hand labeled five hundred to a thousand candidate sentences of each relationship type to obtain a ground truth set (Table {@tbl:candidate-sentences})[^1].

[^1]: Labeled sentences are available [here](https://github.com/greenelab/text_mined_hetnet_manuscript/tree/master/supplementary_materials/annotated_sentences).

| Relationship | Train | Tune | Test |
| :--- | :---: | :---: | :---: |
| Disease Associates Gene | 2.35 M |31K (397+, 603-) | 313K (351+, 649-) |
| Compound Binds Gene | 1.7M | 468K (37+, 463-) | 227k (31+, 469-) |
| Compound Treats Disease | 1.013M | 96K (96+, 404-) | 32K (112+, 388-) |
| Gene Interacts Gene | 12.6M | 1.056M (60+, 440-) | 257K (76+, 424-) |

Table: Statistics of Candidate Sentences. 
We sorted each candidate sentence into a training, tuning and testing set.
Numbers in parentheses show the number of positives and negatives that resulted from the hand-labeling process.
{#tbl:candidate-sentences}

### Label Functions for Annotating Sentences

The challenge of having too few ground truth annotations is common to many natural language processing settings, even when unannotated text is abundant.
Data programming circumvents this issue by quickly annotating large datasets by using multiple noisy signals emitted by label functions [@arxiv:1605.07723].
Label functions are simple pythonic functions that emit: a positive label (1), a negative label (-1) or abstain from emitting a label (0).
We combine these functions using a generative model to output a single annotation, which is a consensus probability score bounded between 0 (low chance of mentioning a relationship) and 1 (high chance of mentioning a relationship).
We used these annotations to train a discriminator model that makes the final classification step.
Our label functions fall into three categories: databases, text patterns and domain heuristics.
We provide examples for each category in our [supplemental methods section](#label-function-categories).  

### Training Models

#### Generative Model

The generative model is a core part of this automatic annotation framework.
It integrates multiple signals emitted by label functions and assigns a training class to each candidate sentence.
This model assigns training classes by estimating the joint probability distribution of the latent true class ($Y$) and label function signals ($\Lambda$), $P(\Lambda, Y)$.
Assuming each label function is conditionally independent, the joint distribution is defined as follows:  

$$
P(\Lambda, Y) = \frac{\exp(\sum_{i=1}^{m} \theta^{T}F_{i}(\Lambda, y))}
{\sum_{\Lambda'}\sum_{y'} \exp(\sum_{i=1}^{m} \theta^{T}F_{i}(\Lambda', y'))}
$$  

where $m$ is the number of candidate sentences, $F$ is the vector of summary statistics and $\theta$ is a vector of weights for each summary statistic.
The summary statistics used by the generative model are as follows:  

$$F^{Lab}_{i,j}(\Lambda, Y) = \unicode{x1D7D9}\{\Lambda_{i,j} \neq 0\}$$
$$F^{Acc}_{i,j}(\Lambda, Y) = \unicode{x1D7D9}\{\Lambda_{i,j} = y_{i,j}\}$$   

*Lab* is the label function's propensity (the frequency of a label function emitting a signal).
*Acc* is the individual label function's accuracy given the training class.
This model optimizes the weights ($\theta$) by minimizing the negative log likelihood:

$$\hat{\theta} = argmin_{\theta} -\sum_{\Lambda} log \sum_{Y}P(\Lambda, Y)$$

In the framework we used predictions from the generative model, $\hat{Y} = P(Y \mid \Lambda)$, as training classes for our dataset [@doi:10.14778/3157794.3157797; @doi:10.1145/3209889.3209898]. 

### Discriminative Model

The generative model produces predicted probabilities for each sentence by integrating output from label functions.
The discriminative model is a neural network trained to produce classification labels by intergrating predicted probabilities from the generative model as well as with sentence representations via word embeddings.
The goal of this combined approach is to develop models that learn text features associated with the overall task that go beyond the supplied label functions.
We used a piecewise convolutional neural network that contains multiple kernel filters as our discriminative model.
We built a network with multiple filters using a fixed width of 300 (size of word embeddings) and a fixed height of 7 (Figure {@fig:convolutional_network}).
We choose a fixed height of 7 because this height was previously reported to optimize performance in relationship classification [@arxiv:1510.03820].
We trained this model for 15 epochs using the Adam optimizer [@arxiv:1412.6980] with pytorch's default parameter settings and a learning rate of 0.001 that decreases by half every epoch until the lower bound of 1e-5 is reached, which we observed was often sufficient for convergence.
We added a L2 penalty (lambda=0.002) on the network weights to prevent overfitting.
Lastly, we added a dropout layer (p=0.25) between the fully connected layer and the softmax layer.

![
The architecture of the discriminative model was a convolutional neural network.
We performed a convolution step using multiple filters. 
The filters generated a feature map that was sent into a maximum pooling layer that was designed to extract the largest feature in each map.
The extracted features were concatenated into a singular vector that was passed into a fully connected network. 
The fully connected network had 300 neurons for the first layer, 100 neurons for the second layer and 50 neurons for the last layer. 
The last step from the fully connected network was to generate predictions using a softmax layer.
](images/figures/convolutional_neural_network/convolutional_neural_nework.png){#fig:convolutional_network}

#### Word Embeddings

Word embeddings are representations that map individual words to real valued vectors of user-specified dimensions.
These embeddings have been shown to capture the semantic and syntactic information between words [@arxiv:1310.4546].
We trained Facebook's fastText [@arxiv:1607.04606] using all candidate sentences for each individual relationship pair to generate word embeddings.
FastText uses a skip-gram model [@arxiv:1301.3781] that aims to predict the surrounding context for a candidate word and pairs the model with a novel scoring function that treats each word as a bag of character n-grams.
We trained this model for 20 epochs using a window size of 2 and generated 300-dimensional word embeddings.
We use the optimized word embeddings as input to our discriminative model. 

#### Calibration of the Discriminative Model

Often many tasks require a machine learning model to output reliable probability predictions. 
A model is well calibrated if the probabilities emitted from the model match the observed probabilities.
For example, a well-calibrated model that assigns a class label with 80% probability should have that class appear 80% of the time.
Deep neural network models can often be poorly calibrated [@arxiv:1706.04599; @arxiv:1807.00263].
These models are usually over-confident in their predictions.
For this reason, we calibrated our convolutional neural network using temperature scaling. 
Temperature scaling uses a parameter T to scale each value of the logit vector (z) before being passed into the softmax (SM) function.

$$\sigma_{SM}(\frac{z_{i}}{T}) = \frac{\exp(\frac{z_{i}}{T})}{\sum_{i}\exp(\frac{z_{i}}{T})}$$

We found the optimal T by minimizing the negative log likelihood (NLL) of the tune set.

### Experimental Design

Being able to re-use label functions across edge types would substantially reduce the number of label functions required to extract multiple relationships from biomedical literature.
We first established a baseline by training a generative model using only distant supervision label functions designed for the target edge type.
As an example, for the GiG edge type we used label functions that returned a `1` if the pair of genes were included in the Human Interaction database [@doi:10.1016/j.cell.2014.10.050], the iRefIndex database [@doi:10.1186/1471-2105-9-405] or in the Incomplete Interactome database [@doi:10.1126/science.1257601].
Then we compared models that also included text and domain-heuristic label functions.
Using a sampling with replacement approach, we sampled these text and domain-heuristic label functions separately within edge types, across edge types, and from a pool of all label functions.
We compared within-edge-type performance to across-edge-type and all-edge-type performance.
For each edge type we sampled a fixed number of label functions consisting of five evenly-spaced numbers between one and the total number of possible label functions.
We repeated this sampling process 50 times for each point.
We evaluated both generative and discriminative (training and downstream analyses are described in the [supplemental methods section](#discriminative-model))  models at each point, and we reported performance of each in terms of the area under the receiver operating characteristic curve (AUROC) and the area under the precision-recall curve (AUPR).
Lastly, we conducted a follow up experiment for the generative model described in the [supplemental methods section](#adding-random-noise-to-generative-model).

## Discussion and Conclusions

Filling out knowledge bases via manual curation can be an arduous and erroneous task [@doi:10.1093/bioinformatics/btm229].
Using manual curation alone becomes impractical as the rate of publications continuously increases.
Data programming is a paradigm that uses label functions to speed up the annotation process and can be used to solve this problem.
However, creating useful label functions is an obstacle to this paradigm, which takes considerable time.
We tested the feasibility of re-using label functions to reduce the number of label functions required for strong prediction performance.

Through our sampling experiment, we found that adding edge-specific label functions increases performance for the generative model (Figure {@fig:auroc_gen_model_test_set}).
We found that label functions designed from relatively related edge types can increase performance (Gene-interacts0Gene (GiG) label functions predicting the Compound-binds-Gene (CbG) edge and vice versa), while the Disease-associates-Gene (DaG) edge type remained agnostic to label function sources (Figure {@fig:auroc_gen_model_test_set} and Supplemental Figure {@fig:aupr_gen_model_test_set}).
Furthermore, we found that using all label functions at once generally hurts performance with the exception being the DaG edge type (Supplemental Figures {@fig:auroc_grabbag_gen_model_test_set} and {@fig:aupr_grabbag_gen_model_test_set}).
One possibility for this observation is that DaG is a broadly defined edge type.
For example, DaG may contain many concepts related to other edge types, such as Disease (up/down) regulating a Gene, which makes it more agnostic to label function sources (examples highlighted in our [annotated sentences](https://github.com/greenelab/text_mined_hetnet_manuscript/tree/master/supplementary_materials/annotated_sentences)).  

Regarding the discriminative model, adding edge-specific label function improved performance for two out of the four edge types (Compound-treats-Disease (CtD) and  DaG) (Figure {@fig:auroc_discriminative_model_performance} and Supplemental Figure {@fig:aupr_discriminative_model_performance}). 
GiG discriminative model showed minor improvements compared to the generative model, but only when nearly all edge-specific label functions are included (Figure {@fig:auroc_discriminative_model_performance} and Supplemental Figure {@fig:aupr_discriminative_model_performance}).
In terms of CbG, the discriminator model failed to improve over the generative model (Figure {@fig:auroc_discriminative_model_performance} and Supplemental Figure {@fig:aupr_discriminative_model_performance}).
A possible explanation for hindered performance is that these edge types contain many sentences with spurious gene mentions, resulting in a lot of false-positive examples.
Improving performance for all edge types would require more labeled examples for the tuning set, along with the construction of more label functions for the generative model.
Even with these limitations, we estimated the recall amount of edges within Hetionet v1.
We found that our approach could recall a modest amount of existing edges while proposing a significant amount of new edges to be included (Figure {@fig:hetionet_reconstruction}). 
These results suggest that text mining is a viable approach to expanding existing knowledge bases; however, more advanced techniques [@doi:10.1145/2623330.2623623] may be necessary to combat the problem of high false positives.

Overall, we conclude that the re-use of label functions is possible for closely related edge types, but it does not improve performance for most pairings.
The discriminative model's performance improves as more edge-specific label functions are incorporated into the generative model; however, performance greatly depends on the annotations provided by the generative model.
This pattern suggests that future endeavors should emphasize label function construction and generative model training over the discriminative model to achieve better downstream performance.
Furthermore, other strategies such as multitask learning [@doi:10.1145/3209889.3209898] or transfer learning [@doi:10.1186/s40537-016-0043-6] could make mining multiple edge types more practical.
